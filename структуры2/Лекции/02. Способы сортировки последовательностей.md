**Сортировка** - это перемещение элементов последовательности в порядке возрастания (неубывания) или убывания (невозрастания)  
Области применения задачи сортировки:
1. Задача группировки
2. Поиск общих элементов в 2 или более последовательностях
3. Задача поиска
  
Задача: необходимо упорядочить $N$ записей $R_1$, $R_2$, $...$, $R_N$. Пусть каждая запись $R_i$ имеет ключ $k_i$, который и управляет процессом сортировки (в дальнейшем будем сортировать только ключи). Введем на множестве ключей отношение порядка $<$, такое, что для любых 3 значений ключа $a, b, c$ выполняются 2 условия:
1. Существует одно и только одно утверждение: либо $a < b$, либо $b < a$, либо $a=b$ (закон трихотомии)
2. Если $a < b$, а $b < c$, то из этого следует, что $a < c$ (закон транзитивности)  
  
Любое множество с данным отношением подлежит сортировке в выбранном нами смысле. Т. е. записи упорядочиваются в ряд $R_{i1}, R_{i2}, ..., R_{in}$, где $k_{i1}≤k_{i2}≤k_{in}$.  
## Сложность алгоритма

Скорость выполнения некоторой программы во многом зависит от быстродействия компьютера. Но даже на самом быстром компьютере одни программы будут работать быстро, а другие либо медленно, либо вообще не выдавать результат за адекватное время. Это связано не с производительностью техники, а со сложностью алгоритма. Под сложностью понимается количество элементарных операций (шагов цикла), которые будут выполнены с исходными данными размером $n$.  
Например, если для решения задачи используются 2 цикла, один из которых вложен в другой, каждый цикл производит $n$ итераций, а тело цикла выполняется всегда за некоторое константное время $C$, то время на выполнение будет пропорционально $C*n^2$. При малых $n$ константа $C$ играет существенное значение. Но чем больше $n$, тем значение константы $C$ уменьшается. Поэтому при оценке сложностей все подобные константы не учитываются. Сложность оценивается сверху (хуже чего не будет) и это описывается О-символикой: $O(f(n))$.  
## Базовые методы сортировки массивов
Среди методов сортировки массивов выделяют 3 базовых:
- Метод прямого выбора 
- Метод прямого включения
- Метод прямого обмена
#### Метод прямого выбора
Среди всех элементов ищется минимальный и меняется местами с первым. Далее минимальный ищется среди оставшихся. и меняется местами со вторым. И т. д.
```cpp
#include <iostream>
#include <fstream>
using namespace std;

void selection(int* m, int n) {
	for (int i = 0; i < n - 1; i++) {
		int min = m[i], index = i;
		for (int j = i + 1; j < n; j++)
			if (m[j] < min) min = m[j], index = j;
		m[index] = m[i];
		m[i] = min;
	}
}

void main() {
	const int n = 10;
	int a[n] = { 10, 4, 55, 66, 4, 123, 12, 666, 3, -1 };
	selection(a, n);
	for (int i = 0; i < n; i++)
		cout << a[i] << ' ';
	cout << endl;
	system("pause");
}
```
Сложность метода - на любом массиве будет $O(n^2)$, т. к. в методе 2 вложенных цикла: первый проходит $n-1$ итерацию, внутренний - $n-1, n-2, ...$ итераций. 
#### Метод прямого включения (метод вставки)
$i$-тый элемент вставляется среди предыдущих на подходящее для него место. Эта процедура проводится для всех элементов массива, начиная со второго и до последнего:
```
1) 4
	10, 10, ...
	4, 10, ...
2) 55
	4, 10, 55, ...
3) 66
	4, 10, 55, 66, ...
4) 4
	4, 10, 55, 66, 66, ...
	4, 10, 55, 55, 66, ...
	4, 10, 10, 55, 66, ...
	4, 4, 10, 55, 66, ...
И т. д.
```
  
```cpp
#include <iostream>
#include <fstream>
using namespace std;

void insertion(int* m, int n) {
	for (int i = 1; i < n; i++) {
		int x = m[i], j = i;
		while (j != 0 && x < m[j - 1]) m[j] = m[j - 1], j--;
		m[j] = x;
	}
}

void main() {
	const int n = 10;
	int a[n] = { 10, 4, 55, 66, 4, 123, 12, 666, 3, -1 };
	insertion(a, n);
	for (int i = 0; i < n; i++)
		cout << a[i] << ' ';
	cout << endl;
	system("pause");
}
```
В худшем случае, как и в среднем, сложность $O(n^2)$ из-за цикла в цикле. Однако в лучшем случае (уже отсортированный массив) сложность $O(n)$, т. к. тело цикла while не будет выполняться ни разу. 
#### Метод прямого обмена (метод пузырька)
Последний элемент массива сравнивается с предпоследним и если последний меньше, они меняются местами. Предпоследний элемент массива сравнивается с третьим с конца и снова упорядочивается. И т. д. до начала массива. В результате на 1 месте окажется минимальный элемент. Процедура повторяется, но движемся до 2, 3, ... элементов.  
Алгоритм называется методом пузырька, т. к. если представить массив вертикальным, то элементы, имеющие меньшие значения, на каждом проходе, как легкие пузырьки в воде, поднимаются вверх.
```cpp
#include <iostream>
#include <fstream>
using namespace std;

void bubble(int* m, int n) {
	for (int i = 0; i < n - 1; i++) {
		for (int j = n - 1; j > i; j--)
			if (m[j] < m[j - 1]) swap(m[j], m[j - 1]);
	}
}

void main() {
	const int n = 10;
	int a[n] = { 10, 4, 55, 66, 4, 123, 12, 666, 3, -1 };
	bubble(a, n);
	for (int i = 0; i < n; i++)
		cout << a[i] << ' ';
	cout << endl;
	system("pause");
}
```
2 вложенных цикла for дают сложность на любом входном массиве $O(n^2)$. Среди 3 базовых методов этот самый медленный.
## Улучшенные методы сортировки массивов
#### Шейкерная сортировка
Данный метод является улучшением метода пузырька:
1. Запоминать, были или нет перестановки в процессе некоторого прохода. Если не было - сортировку можно завершить
2. Запоминать не только сам факт перестановки, но и место последнего обмена. Ясно, что после этого места массив уже отсортирован.
3. Проводить сортировку последовательно в 2 направлениях по массиву - сначала от конца к началу, а затем от начала к концу
```cpp
#include <iostream>
#include <fstream>
using namespace std;

void shaker(int* m, int n) {
	int left = 0, right = n - 1, k = right;
	do {
		for (int j = right; j >= left; j--)
			if (m[j] < m[j - 1]) {
				swap(m[j], m[j - 1]);
				k = j;
			}
		left = k + 1;

		for (int j = left; j <= right; j++)
			if (m[j] < m[j - 1]) {
				swap(m[j], m[j - 1]);
				k = j;
			}
		right = k - 1;
	} while (left <= right);
}

void main() {
	const int n = 10;
	int a[n] = { 10, 4, 55, 66, 4, 123, 12, 666, 3, -1 };
	shaker(a, n);
	for (int i = 0; i < n; i++)
		cout << a[i] << ' ';
	cout << endl;
	system("pause");
}
```
В лучшем случае на уже отсортированном массиве сложность будет $O(n)$, т. к. будет выполнен 1 раз первый цикл for, после чего **left** станет равным **right**. Однако в худшем случае (массив отсортирован в обратном порядке) сложность все равно будет $O(n^2)$.
#### Сортировка Шелла
Сначала отдельно группируются и сортируются элементы, отстоящие друг от друга на расстоянии $n/2$. Затем на расстоянии $n/4$ и так далее, пока не дойдем до обычной одинарной сортировки. На каждом проходе сортировка программируется как сортировка вставками, поэтому если какая-то последовательность уже отсортирована, происходит переход в следующий.

10, 4, 55, 66, 4, 123, 12, 666  
<span style="color:#ff0000">4</span>, <span style="color:#00b050">4</span>, <span style="color:#ffff00">12</span>, 66, <span style="color:#ff0000">10</span>, <span style="color:#00b050">123</span>, <span style="color:#ffff00">55</span>, 666  
<span style="color:#ff0000">4</span>, <span style="color:#0070c0">4</span>, <span style="color:#ff0000">10</span>, <span style="color:#0070c0">66</span>, <span style="color:#ff0000">12</span>, <span style="color:#0070c0">123</span>, <span style="color:#ff0000">55</span>, <span style="color:#0070c0">666</span>  
4, 4, 10, 12, 55, 66, 123, 666  

```cpp
#include <iostream>
#include <fstream>
using namespace std;

void shell(int* m, int n) {
	int step = n / 2;
	while (step > 0) {
		for (int i = 0; i < n - step; i++) {
			int j = i;
			while (j >= 0 && m[j] > m[j + step]) {
				swap(m[j], m[j + step]);
				j -= step;
			}
		}
		step /= 2;
	}
}

void main() {
	const int n = 8;
	int a[n] = { 10, 4, 55, 66, 4, 123, 12, 666 };
	shell(a, n);
	for (int i = 0; i < n; i++)
		cout << a[i] << ' ';
	cout << endl;
	system("pause");
}
```
Сортировка дает выигрыш по сравнению с классическими методами, потому что на каждом шаге сортируется либо немного элементов (2, 4, 8, ...), либо элементы уже почти отсортированы. В классической сортировке Шелла расстояние между элементами меняется как числа, кратные 2. На самом деле, это не лучший вариант. До сих пор окончательно неизвестно, при каких расстояниях в сериях скорость сортировки самая быстрая. Но точно известно, что они не должны быть множителями друг друга. "Хороший" результат дают серии $(1, 4, 13, 40, 121, ...)$ или $(1, 3, 7, 15, 31, ...)$. При этом, сложность метода при "хорошем" распределении метода пропорциональна $O(n^{1,2})$.
#### Пирамидальная сортировка
Одномерный массив представляется в виде бинарного дерева. У нулевого элемента потомки под номерами 1 и 2, у первого - 3 и 4 и так далее.  
![[02_01. Пирамидальная сортировка - шаг 1.png]]  
Затем все узлы дерева выстраиваются в таком порядке, что никакой родитель не меньше своих прямых потомков  
![[02_02. Пирамидальная сортировка - шаг 2.png]]  
В результате такой процедуры в корне дерева (нулевой элемент массива) появляется наибольший элемент. Поменяем его местами с последним.  
![[02_03. Пирамидальная сортировка - шаг 3.png]]  
В результате все элементы дерева находятся на своих местах, за исключением нулевого элемента. Его нужно протолкнуть по дереву до требуемого места.  
![[02_04. Пирамидальная сортировка - шаг 4.png]]  
После этого в корне дерева вновь окажется максимальный из оставшихся. Поменяем его местами с предпоследним и т. д.  
Выстраивание дерева в "хорошем" виде (когда ни один родитель не меньше детей) требует одноразового прохода по всем элементам. Сложность - $O(n)$.  
В дальнейшем каждый элемент проталкиваем по дереву, затрачивая на это не более $log_2n$. Всего элементов $n$, поэтому сложность - $O(n∗log_2n)$.  
Итоговая сложность - $O(n)+O(n∗log_2n)$, но $O(n)$ при больших $n$ можно пренебрегнуть, поэтому на любых массивах сложность будет $O(n∗log_2n)$.  
```cpp
#include <iostream>
#include <fstream>
using namespace std;

void pushDown(int* m, int root, int bottom) {
	int done = 0, maxChild;
	while ((root * 2 + 1 <= bottom) && (!done)) {
		if (root * 2 + 1 == bottom) maxChild = root * 2 + 1;
		else if (m[root * 2 + 1] > m[root * 2 + 2]) maxChild = root * 2 + 1;
		else maxChild = root * 2 + 2;
		if (m[root] < m[maxChild]) {
			swap(m[root], m[maxChild]);
			root = maxChild;
		}
		else done = 1;
	}
}

void heapSort(int* m, int n) {
	for (int i = n / 2 - 1; i >= 0; i--)
		pushDown(m, i, n - 1);
	for (int i = n - 1; i >= 0; i--) {
		swap(m[0], m[i]);
		pushDown(m, 0, i - 1);
	}
}

void main() {
	const int n = 8;
	int a[n] = { 10, 4, 55, 66, 4, 123, 12, 666 };
	heapSort(a, n);
	for (int i = 0; i < n; i++)
		cout << a[i] << ' ';
	cout << endl;
	system("pause");
}
```
#### Быстрая сортировка (quicksort, сортировка Хоара)
В массиве выбирается так называемый **опорный элемент**, стоящий в его центре ($n/2$). После этого все элементы массива переставляются таким образом, что слева оказываются элементы, меньшие либо равные опорному, а справа - большие либо равные опорному. После этого сортировка рекурсивно применяется к левой и правой частям массива и т. д. Обмен элементами производится следующим образом: слева ищется элемент, не меньший опорного, справа - не больший, меняются местами и т. д.  
$L = 0$, $R = 7$, $key = 4$  
<span style="color:#0070c0">8</span>, 4, 55, 66, <span style="color:#ff0000">4</span>, 123, 12, 666  
$L = 0$, $R = 4$  
4, 4, 55, 66, 8, 123, 12, 666  
$L = 1$, $R = 1$  
```cpp
#include <iostream>
#include <fstream>
using namespace std;

void sort(int* m, int left, int right) {
	int i = left, j = right, key = m[(left + right) / 2];
	do {
		while (m[i] < key) i++;
		while (m[j] > key) j--;
		if (i <= j) {
			swap(m[i], m[j]);
			i++;
			j--;
		}
	} while (i <= j);
	if (i < right) sort(m, i, right);
	if (j > left) sort(m, left, j);
}

void quickSort(int* m, int n) {
	sort(m, 0, n - 1);
}

void main() {
	const int n = 8;
	int a[n] = { 10, 4, 55, 66, 4, 123, 12, 666 };
	quickSort(a, n);
	for (int i = 0; i < n; i++)
		cout << a[i] << ' ';
	cout << endl;
	system("pause");
}
```
Если ключевой элемент выбирается таким образом, что все элементы массива делятся им по значениям меньше/больше ровно пополам, то левая и правая части относительно ключа будут одинаковы и количество таких делений $log_2n$. При этом на каждом проходе пробегаем по всем элементам. Следовательно, сложность - $O(n∗log_2n)$. Но в худшем случае, если ключом оказывается минимальный или максимальный элемент, то только он окажется на своем месте, а остальные элементы массива либо справа от него (если ключ минимальный), либо слева (если ключ максимальный). И если так каждый раз, то сложность будет $O(n^2)$.
#### Карманная сортировка
Это единственная сортировка, которая может упорядочить элементы за время $O(n)$. Но для этого множество всех возможных значений ключа должно быть ограничено и мы это ограничение должны знать. Для сортировки заводится массив "карманов". Каждый элемент массива ("карман") - это указатель на список, в котором расположены объекты с ключом, совпадающим с индексом кармана.  
![[02_05. Карманная сортировка.png]]  
## Внешняя сортировка
Файлы устроены таким образом, что в каждый момент времени доступна только одна компонента файла. Поэтому для данных, находящихся в файле, стандартные методы сортировки не подходят. В отличие от оперативной памяти, размер внешней существенно больше, и при сортировке можно использовать дополнительные контейнеры. На этом и основаны большинство подходов внешней сортировки.
#### Прямое слияние
1. Последовательность $a$ разбивается на 2 половины - $b$ и $c$
2. Части $b$ и $c$ сливаются в одну последовательность, при этом одиночные элементы из 2 частей образуют упорядоченную пару
3. Полученная последовательность под именем $a$ вновь делится пополам на $b$ и $c$ и далее выполняется п. 2, но упорядоченные пары из каждой части уже сливаются в упорядоченные четверки. Далее вновь разделение и четверки сливаются в восьмерки и т. д. Так до тех пор, пока не получится упорядоченная последовательность $a$
  
```
a: 8, 4, 55, 66, 4, 123, 12, 666

b: 8, 4, 55, 66
c: 4, 123, 12, 666
a: 4, 8, 4, 123, 12, 55, 66, 666

b: 4, 8, 4, 123
c: 12, 55, 66, 666
a: 4, 8, 12, 55, 4, 66, 123, 666

b: 4, 8, 12, 55
c: 4, 66, 123, 666
a: 4, 4, 8, 12, 55, 66, 123, 666
```
Действие по однократной обработке всех элементов последовательности называется **фазой**. Наименьший процесс, повторение которого составляет сортировку, называется **проходом**. В примере сортировка произведена за 3 прохода, каждый из которых состоит из фазы разделения и слияния. Поскольку на каждом проходе размер сливаемых групп увеличивается в 2 раза, то таких проходов $log_2n$, при этом каждый раз дважды проходятся все элементы. А значит сложность - $O(n∗log_2n)$.  
Можно заметить, что фаза разделения к самому процессу сортировки (сравнения элементов) отношения не имеет. Поэтому сортировку можно усовершенствовать, сливая группы не в 1 файл $a$, а в 2 по очереди. Эти 2 вновь полученных файла и будут исходными для вновь сливаемых последовательностей:
```
a: 8, 4, 55, 66, 4, 123, 12, 666

b: 8, 4, 55, 66
c: 4, 123, 12, 666

d: 4, 8, 4, 123
e: 12, 55, 66, 666

b: 4, 4, 8, 123
c: 12, 55, 66, 666

d: 4, 4, 8, 12, 55, 66, 123, 666
e:
```
#### Естественное слияние
В случае прямого слияния мы не получаем никакого преимущества, даже если последовательность уже частично или полностью отсортирована. На самом деле можно делить файлы на последовательности разной длины, если эти последовательности уже отсортированы естественным образом. В этом случае сразу сливаются не одиночные элементы, а наибольшие по длине уже отсортированные последовательности
```
a: 8, 4, 55, 66, 4, 123, 12, 666
a: 4, 8, 55, 66, 4, 12, 123, 666
a: 4, 4, 8, 12, 55, 66, 123, 666
```
#### Многопутевое слияние
Очевидно, что количество проходов можно уменьшить, если увеличить количество файлов, на которые будем делить исходный. Если их $k$, то количество проходов - $log_kn$, а сложность - $O(n∗log_kn)$. 
#### Многофазное слияние
В основе метода лежит идея отказа от понятия прохода. Пусть у нас имеется 2 файла, в первом из которых $n_1$ естественных серий, а во втором - $n_2$, причем $n_1>n_2$. Будем сливать серии в третий файл, пока во втором они не закончатся. Теперь станем сливать первый и третий во второй и т. д.

| $F1$ | $F2$ | $F3$ |
| ---- | ---- | ---- |
| 13   | 8    | 0    |
| 5    | 0    | 8    |
| 0    | 5    | 3    |
| 3    | 2    | 0    |
| 1    | 0    | 2    |
| 0    | 1    | 1    |
| 1    | 0    | 0    |
  
Нетрудно заметить, что количество строк в таблице, а значит и количество действий по слиянию сильно зависит от количества серий в 2 исходных файлах $F1$ и $F2$ (например, в $F1$ - 20, в $F2$ - 1). Лучший результат получается, когда исходное количество серий в файлах $F1$ и $F2$ являются соседними числами Фибоначчи. 